---
title: "Ridge Regression Problems"
output: html_document
---
# extra problem 1
```{r}
library(car)
setwd("/Users/cs0823/Documents/STA_207/dataset")
data = read.table(file = "apartment_data.csv", header= FALSE, sep = ",")
colnames(data)[colnames(data)=="V1"] <- "Y"
colnames(data)[colnames(data)=="V2"] <- "X1"
colnames(data)[colnames(data)=="V3"] <- "X2"
colnames(data)[colnames(data)=="V4"] <- "X3"
colnames(data)[colnames(data)=="V5"] <- "X4"
colnames(data)[colnames(data)=="V6"] <- "X5"
```

## (a) Obtain a matrix plot of the data
```{r}
cor_matrix = cor(data, method = c("pearson"))
cor_matrix
```
> From the correlation matrix we can conclude that:
1. Y is highly correlated with X2, X3, and X5. 
2. X predictors (X2, X3, X5) are highly correlated with each other, which indicates multicollinearity.

## (b) Obtain the eigenvalues of the matrix X'X
```{r}
X = as.matrix(data[,2:6])
X = scale(X)
XtX = t(X) %*% X

# eigen values and vectors
eigen_values = eigen(XtX)$values
eigen_values

# The K first eigen values explain about the percentage of the total variability of the independent
sum(eigen_values[1])/sum(eigen_values)
sum(eigen_values[1:2])/sum(eigen_values)
sum(eigen_values[1:3])/sum(eigen_values)
sum(eigen_values[1:4])/sum(eigen_values)
sum(eigen_values[1:5])/sum(eigen_values)
```
> From the results above, the first two eigen vectors can explain 79% variability of Y, which implies the multicollinearity among the X variables. 

## (c) Run a multiple regression model, obtain the parameters, their standard errors and the ANOVA table
```{r}
model = lm(Y ~ X1+X2+X3+X4+X5, data)
anova(model)
summary(model)
```
> we can conclude the model in the following form:
Y = -0.8550X1 + 4.1471X2 + 0.0009X3 + 2.6996X4 + 0.01555X5 + 92.6238

## (d) Obtain the variance-inÃ¡ation factors for each of the independent variables.
```{r}
vif(model)
```
> We can conclude that VIF of X2 is high = 7.657888, which indicates intercorrelation between X2 and the other
X variables.

## (e) Use the generalized cross-validation criterion to select the penalty for the ridge regression. 
```{r}
par(mfrow=c(1,2))
Y = as.vector(data[,1])
n = nrow(data)
X = as.matrix(scale(data[,2:6]))
I = diag(5)

# first trial 
K = seq(1, 100, 1)
GCV = c()
for (i in 1:length(K)) {
  k = K[i]
  Hat_Matrix_k = X %*% solve(XtX + k*I) %*% t(X)
  SSE_k = sum((Y- Hat_Matrix_k%*%Y)^2) 
  h_bar = sum(diag(Hat_Matrix_k))/n
  GCV[i] = (1/n) *(SSE_k) * (1-h_bar )^(-2)
}
plot(K, GCV)
# The minimum is between 34 to 36

# Second trial
K = seq(34, 36, 0.01)
GCV = c()
for (i in 1:length(K)) {
  k = K[i]
  Hat_Matrix_k = X %*% solve(XtX + k*I) %*% t(X)
  SSE_k = sum( (Y-Hat_Matrix_k%*%Y)^2 ) 
  h_bar = sum(diag(Hat_Matrix_k))/n
  GCV[i] = (1/n) *(SSE_k) * (1-h_bar)^(-2)
}
plot(K, GCV)

# Estimated parameter
k=34.3 # from the plot of second trial
Hat_Matrix_k = X %*% solve(XtX + k*I) %*% t(X)
beta_hat = solve(XtX + k*I) %*% t(X)  %*% Y
beta_hat

# estimated standard errors 
sigma_square_hat= sum((Y-Hat_Matrix_k%*%Y)^2)/n
para_var = sigma_square_hat*solve(XtX + k*I)
diag(para_var)
```
> 1. The estimated optimal K = 34.3
  2. The estimated beta is provided below:
  X1 -6.415761
  X2 48.196467
  X3 31.502113
  X4  9.912591
  X5 57.909677
  3. Variance of beta hat:
        X1       X2       X3       X4       X5 
    1627.463 1956.717 1846.605 1632.127 1880.291 
    
## (f) Plot the observed response against the fitted Y values. Also plot the residuals against the fitted, and the
normal probability plot of the residuals.
```{r}
par(mfrow=c(1,3))
fitted_Y = Hat_Matrix_k%*%Y
plot(fitted_Y, Y, main = "fitted_Y values vs. Y values")

res= Y - fitted_Y 
plot(fitted_Y, res, main = "fitted_Y values vs. Residauls")

qqnorm(res)
qqline(res)
```
> Plot of fitted_Y versus Y is linear, however, the normality assumption does not seem to be valid. The plot of residuals against the fitted values of Y indicates there existed serious tail problem. 

# extra problem 3
## Load required packages 
```{r}
library('readxl')
library('car')
library('leaps')
library('SuppDists')
library('ALSM')
library('MASS')
```

## (a)
```{r}
lambda=c(19,3,1,0.7,0.3)
sigma_sq=2.5
n=25
etb=c(0.8,0.3,0.2,0.2,0.1)
Dfun=function(k) sigma_sq*sum( lambda/(k+lambda)^2 )+k^2*sum( etb^2/(k+lambda)^2   )
optim(1,Dfun, method="L-BFGS-B",lower=0,upper=10000)
```
> The actual minima is K = 11.9484. Since K needs to be a postive integer, we round the value and get K = 12.

## (b) 
```{r}
Lfun=function(k) sigma_sq*sum( lambda^2/(k+lambda)^2)+k^2*sum(etb^2*lambda/(k+lambda)^2)
optim(1,Lfun, method="L-BFGS-B",lower=0,upper=10000)
```
> Similarly, the actual minima is K = 6.17, we round it up and get K = 6.

## (c)
```{r}
Lfun(0)
Dfun(0)
Lfun(6.179605)
Dfun(11.9484)

```
> When K = 0, we have D = 15.36967, and L = 12.5. These numbers are not exactly the same, but similar.
  Lfun(6.179605) = 2.679957, and Dfun(11.9484) = 0.346172



